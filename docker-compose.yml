services:
  postgres:
    image: pgvector/pgvector:pg16
    container_name: cartulary-postgres
    environment:
      POSTGRES_DB: cartulary
      POSTGRES_USER: cartulary
      POSTGRES_PASSWORD: ${DB_PASSWORD:-changeme}
    volumes:
      - postgres_data:/var/lib/postgresql/data
      - ./init-db.sql:/docker-entrypoint-initdb.d/init.sql
    ports:
      - "5432:5432"
    healthcheck:
      test: ["CMD-SHELL", "pg_isready -U cartulary"]
      interval: 10s
      timeout: 5s
      retries: 5
    networks:
      - cartulary-network

  redis:
    image: redis:7-alpine
    container_name: cartulary-redis
    ports:
      - "6379:6379"
    volumes:
      - redis_data:/data
    healthcheck:
      test: ["CMD", "redis-cli", "ping"]
      interval: 10s
      timeout: 3s
      retries: 3
    networks:
      - cartulary-network

  backend:
    build:
      context: ./apps/backend
      dockerfile: Dockerfile
    container_name: cartulary-backend
    command: uvicorn app.main:app --host 0.0.0.0 --port 8000 --reload
    volumes:
      - ./apps/backend:/app
      - ./document_storage:/data/documents
      - ./import_watch:/data/import_watch
    # Backend is not exposed to host - access via frontend proxy on port 8080
    # Uncomment below for direct backend access during debugging:
    # ports:
    #   - "8000:8000"
    environment:
      - RUN_MIGRATIONS=true
      - DATABASE_URL=postgresql://cartulary:${DB_PASSWORD:-changeme}@postgres:5432/cartulary
      - REDIS_URL=redis://redis:6379/0
      - CELERY_BROKER_URL=redis://redis:6379/0
      - CELERY_RESULT_BACKEND=redis://redis:6379/0
      - SECRET_KEY=${SECRET_KEY:-dev-secret-key-change-in-production}
      - DEBUG=true
      - BACKEND_CORS_ORIGINS=["http://localhost:8080"]
      - EMBEDDING_ENABLED=${EMBEDDING_ENABLED:-false}
      - EMBEDDING_PROVIDER=${EMBEDDING_PROVIDER:-local}
      - EMBEDDING_MODEL=${EMBEDDING_MODEL:-all-MiniLM-L6-v2}
      - EMBEDDING_DIMENSION=${EMBEDDING_DIMENSION:-384}
      - OPENAI_API_KEY=${OPENAI_API_KEY:-}
      - LLM_ENABLED=${LLM_ENABLED:-false}
      - LLM_PROVIDER=${LLM_PROVIDER:-openai}
      - LLM_MODEL=${LLM_MODEL:-gpt-4o-mini}
      - LLM_BASE_URL=${LLM_BASE_URL:-}
      - GEMINI_API_KEY=${GEMINI_API_KEY:-}
      - OIDC_ENABLED=${OIDC_ENABLED:-false}
      - OIDC_DISCOVERY_URL=${OIDC_DISCOVERY_URL:-}
      - OIDC_CLIENT_ID=${OIDC_CLIENT_ID:-}
      - OIDC_CLIENT_SECRET=${OIDC_CLIENT_SECRET:-}
      - OIDC_REDIRECT_URI=${OIDC_REDIRECT_URI:-http://localhost:8080/auth/callback}
      - OIDC_MOBILE_CLIENT_ID=${OIDC_MOBILE_CLIENT_ID:-}
      # OCR configuration
      - OCR_ENABLED=${OCR_ENABLED:-false}
      - OCR_PROVIDER=${OCR_PROVIDER:-auto}
      - OCR_LANGUAGES=${OCR_LANGUAGES:-["en"]}
      - OCR_USE_GPU=${OCR_USE_GPU:-false}
      # Disable OneDNN/MKL optimizations to avoid PaddlePaddle runtime errors
      - FLAGS_use_mkldnn=false
      # Background workers configuration
      - ENABLE_DIRECTORY_WATCHER=${ENABLE_DIRECTORY_WATCHER:-true}
      - ENABLE_IMAP_WATCHER=${ENABLE_IMAP_WATCHER:-false}
    depends_on:
      postgres:
        condition: service_healthy
      redis:
        condition: service_healthy
    networks:
      - cartulary-network

  celery_worker:
    build:
      context: ./apps/backend
      dockerfile: Dockerfile.paddleocr
    container_name: cartulary-celery-worker
    # Run both worker and beat in the same container
    # Limit concurrency to 1 to reduce memory usage (PaddleOCR models are ~4GB per worker)
    command: >
      sh -c "celery -A app.tasks.celery_app beat --loglevel=info --detach &&
             celery -A app.tasks.celery_app worker --loglevel=info --concurrency=1 --max-tasks-per-child=5"
    volumes:
      - ./apps/backend:/app
      - ./document_storage:/data/documents
      - paddleocr_models:/root/.paddleocr  # Cache PaddleOCR models
      - huggingface_models:/root/.cache/huggingface  # Cache sentence-transformers models
    environment:
      - DATABASE_URL=postgresql://cartulary:${DB_PASSWORD:-changeme}@postgres:5432/cartulary
      - REDIS_URL=redis://redis:6379/0
      - CELERY_BROKER_URL=redis://redis:6379/0
      - CELERY_RESULT_BACKEND=redis://redis:6379/0
      - SECRET_KEY=${SECRET_KEY:-dev-secret-key-change-in-production}
      - EMBEDDING_ENABLED=${EMBEDDING_ENABLED:-false}
      - EMBEDDING_PROVIDER=${EMBEDDING_PROVIDER:-local}
      - EMBEDDING_MODEL=${EMBEDDING_MODEL:-all-MiniLM-L6-v2}
      - EMBEDDING_DIMENSION=${EMBEDDING_DIMENSION:-384}
      - OPENAI_API_KEY=${OPENAI_API_KEY:-}
      - LLM_ENABLED=${LLM_ENABLED:-false}
      - LLM_PROVIDER=${LLM_PROVIDER:-openai}
      - LLM_MODEL=${LLM_MODEL:-gpt-4o-mini}
      - LLM_BASE_URL=${LLM_BASE_URL:-}
      - GEMINI_API_KEY=${GEMINI_API_KEY:-}
      # OCR configuration
      - OCR_ENABLED=${OCR_ENABLED:-false}
      - OCR_PROVIDER=${OCR_PROVIDER:-auto}
      - OCR_LANGUAGES=${OCR_LANGUAGES:-["en"]}
      - OCR_USE_GPU=${OCR_USE_GPU:-false}
      # Disable OneDNN/MKL optimizations to avoid PaddlePaddle runtime errors
      - FLAGS_use_mkldnn=false
    deploy:
      resources:
        limits:
          memory: 6G  # PaddleOCR models need ~4GB, plus overhead
        reservations:
          memory: 2G
    depends_on:
      - postgres
      - redis
      - backend
    networks:
      - cartulary-network

  frontend:
    build:
      context: .
      dockerfile: apps/web/Dockerfile
      target: development
    container_name: cartulary-frontend
    ports:
      - "8080:8080"
    volumes:
      # Mount source files for hot reload
      - ./apps/web/src:/app/apps/web/src
      - ./apps/web/vite.config.ts:/app/apps/web/vite.config.ts
      - ./packages/shared/src:/app/packages/shared/src
    environment:
      # Configure Vite dev server proxy to reach backend container
      - VITE_PROXY_TARGET=http://backend:8000
      - VITE_DOCKER=true
    networks:
      - cartulary-network
    depends_on:
      - backend

volumes:
  postgres_data:
  redis_data:
  document_storage:
  paddleocr_models:
  huggingface_models:

networks:
  cartulary-network:
    driver: bridge
